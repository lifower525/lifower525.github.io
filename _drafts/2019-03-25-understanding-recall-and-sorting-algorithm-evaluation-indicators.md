---
layout: post
title: 理解搜索召回排序算法评价指标
category: [machine-learning]
author: Howie Lee
tags: [Rank,mAP,MRR,nDCG]
---

## 摘要
**搜索质量评估**是搜索技术研究的基础性工作，也是核心工作之一，评价在搜索技术研发中扮演着重要角色。

搜索结果的好坏与否主要体现在相关性上。相关性的定义包括狭义和广义两方面:

* 狭义的解释是：检索结果和用户查询的相关程度。
* 广义的解释是：用户查询的综合满意度。

直观的来看，从用户进入搜索框的那一刻起，到需求获得满足为止，这之间经历的过程越顺畅，越便捷，搜索相关性就越好

> **搜索与一般应用的区别**  
> 搜索 `Rank` 是一个比较常见但是也相对特殊的场景。  
> 跟大多数的机器学习算法的应用场景不同的是，搜索更多的关注的是检索结果中的排序的质量，用户往往关注的只是搜索结果中 top-k 的返回列表，而其他的大多数场景往往关注的是召回的每个个体的好坏。

## 常见排序算法评价指标

### 2.1 MAP指标（Mean Average Precision）
MAP 是平均准确率的简称。单个主题的平均准确率是每篇相关文档检索出后的准确率的平均值，主集合的平均准确率(MAP)是每个主题的平均准确率的平均值。（对准确率求了两次平均）

MAP 是反映系统在全部相关文档上性能的单值指标。系统检索出来的相关文档越靠前 (rank 越高)，MAP 就应该越高。如果系统没有返回相关文档，则准确率默认为 0。

1）P（Precision）单query准确率： 返回结果中相关文档占的比例，P=返回结果中相关文档数目/返回结果总数。

2）AP（Average Precision）单query平均准确率：对检索返回的每篇相关文档的准确率求平均，也即对不同召回率点上的正确率求平均。对一个query而言，检索系统返回的结果必然是有序的，而且越相关的文档排的越靠前越好。计算方法如下式：  
$$
\text { Ave } \mathrm{P}=\frac{\sum_{k=1}^{N}(P(k) \times \operatorname{rel}(k))}{\text {number of relevant documents}}=\frac{\sum_{\mathrm{k}=1}^{N}\left(\frac{k}{r_{k}} \times \operatorname{rel}(k)\right)}{N}
$$


$$
\mathrm{MRR} = \frac 1 Q \sum_{i=1}^{Q} \frac 1 {rank_i}
$$